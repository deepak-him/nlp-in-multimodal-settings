{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f1e1c2ad",
   "metadata": {},
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdb3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68117861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/abhilashchauhan/anaconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/Users/abhilashchauhan/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae7cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Viterbi algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). It was introduced by Andrew Viterbi in 1967 for decoding convolutional codes over noisy digital communication links. Since then, it has found widespread use in various areas such as digital communications, speech recognition, bioinformatics, and natural language processing.\n",
      "\n",
      "The algorithm operates by constructing a trellis (a graph that organizes the possible states in a temporal sequence) for the sequence of observed events and then finding the path through this trellis that has the highest probability, taking into account both the probabilities of transitions between states and the probabilities of observations given the states. This path represents the sequence of states that most likely explains the sequence of observations.\n",
      "\n",
      "The key steps of the Viterbi algorithm include:\n",
      "\n",
      "Initialization: At the first time step, the initial probabilities of being in each state are used to initialize the algorithm.\n",
      "\n",
      "Recursion: For each subsequent time step, the algorithm calculates the probability of each state being the end of a path from the start state through the previous time step and then entering the current state. It keeps track of the path that has the highest probability at each step.\n",
      "\n",
      "Termination: At the final time step, the algorithm identifies the state with the highest probability as the end of the most likely path.\n",
      "\n",
      "Path Backtracking: Starting from the identified end state, the algorithm traces back through the trellis using pointers that were stored during the recursion step to find the sequence of states that constitutes the Viterbi path.\n",
      "\n",
      "The Viterbi algorithm is efficient, requiring time and space that are both linear in the number of time steps and the number of states, making it practical for a wide range of applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "The Viterbi algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). It was introduced by Andrew Viterbi in 1967 for decoding convolutional codes over noisy digital communication links. Since then, it has found widespread use in various areas such as digital communications, speech recognition, bioinformatics, and natural language processing.\n",
    "\n",
    "The algorithm operates by constructing a trellis (a graph that organizes the possible states in a temporal sequence) for the sequence of observed events and then finding the path through this trellis that has the highest probability, taking into account both the probabilities of transitions between states and the probabilities of observations given the states. This path represents the sequence of states that most likely explains the sequence of observations.\n",
    "\n",
    "The key steps of the Viterbi algorithm include:\n",
    "\n",
    "Initialization: At the first time step, the initial probabilities of being in each state are used to initialize the algorithm.\n",
    "\n",
    "Recursion: For each subsequent time step, the algorithm calculates the probability of each state being the end of a path from the start state through the previous time step and then entering the current state. It keeps track of the path that has the highest probability at each step.\n",
    "\n",
    "Termination: At the final time step, the algorithm identifies the state with the highest probability as the end of the most likely path.\n",
    "\n",
    "Path Backtracking: Starting from the identified end state, the algorithm traces back through the trellis using pointers that were stored during the recursion step to find the sequence of states that constitutes the Viterbi path.\n",
    "\n",
    "The Viterbi algorithm is efficient, requiring time and space that are both linear in the number of time steps and the number of states, making it practical for a wide range of applications.\n",
    "\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1df528",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarizer(text, max_length=200, min_length=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a60fed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Viterbi algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states . It was introduced by Andrew Vitbi in 1967 for decoding convolutional codes over noisy digital communication links . The algorithm operates by constructing a trellis (a graph that organizes the possible states in a temporal sequence) for the sequence of observed events .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bbe2a",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa91c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a4d5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "text = \"what is data\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to('cpu')\n",
    "max_length = 50\n",
    "\n",
    "output_sequence = model.generate(input_ids=input_ids, max_length=max_length, temperature=1.5,\n",
    "                                top_p=0.92, repetition_penalty=1.2, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c33258",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequence = tokenizer.decode(output_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8902340",
   "metadata": {},
   "source": [
    "## BERT for Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76bc7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e862f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0564d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf157605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "text = \"BERT stands for Bidirectional Encoder Representations from Transformers.\"\n",
    "question = \"What does BERT stand for?\"\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].tolist()[0]\n",
    "\n",
    "ans_start_scores, ans_end_scores = model(**inputs, return_dict=False)\n",
    "ans_start = torch.argmax(ans_start_scores)\n",
    "ans_end = torch.argmax(ans_end_scores) + 1\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[ans_start:ans_end]))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26885cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abhilashchauhan/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77046fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
